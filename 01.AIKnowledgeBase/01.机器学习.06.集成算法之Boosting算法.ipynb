{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Boosting算法系列\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Boosting算法\n",
    "## 1.1.提升算法介绍\n",
    "提升是一个机器学习技术，可以用于回归和分类问题，它每一步产生一个弱预测模型，如决策树，并加权累加到总模型中；如果每一步的弱预测模型生成都是依据损失函数的梯度方向，则称之为梯度提升Gradient boosting。梯度提升算法首先给定一个目标损失函数，它的定义域是所有可行的若函数集合(基函数)；提升算法通过迭代的选择一个负梯度方向上的基函数来逐渐逼近局部最小值。Boosting集合多个’base’分类器从而使它的性能比任何单个base分类器都好地多。即使base分类器的性能比随机猜测稍微好一点（因此base分类器也叫做weak learners），Boosting依旧会得到一个很好地预测结果。Boosting最初的目的是解决分类问题，现在它也可以解决回归问题。提升就是指每一步我都产生一个弱预测模型，然后加权累加到总模型中，然后每一步弱预测模型生成的的依据都是损失函数的负梯度方向，这样若干步以后就可以达到逼近损失函数局部最小值的目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.理论意义\n",
    "如果一个问题存在弱分类器，则可以通过提升的办法得到强分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.算法分类\n",
    "提升算法主要包括GBDT、XGBoost以及AdaBoost三个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.Boosting和Bagging的区别\n",
    "Boosting的base分类器是按顺序训练的，训练每个base分类器时所使用的训练集是加权重的，而训练集中的每个样本的权重系数取决于前一个base分类器的性能。前一个base分类器错误分类的样本点，在下一个base分类器训练时会有一个更大的权重。一旦训练完所有的base分类器，我们组合所有的分类器给出最终的预测结果。最重要的区别是Bagging的每一个学习器都是独立的，不相关的；而Boosting的每个学习器都是根据上一个学习器学习而来的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.Boosting算法总结\n",
    "![images](images/01_06_001.png)<br/>\n",
    "- 给定初始训练数据，由此训练出第一个基学习器；\n",
    "- 根据基学习器的表现对样本进行调整，在之前学习器做错的样本上投入更多关注；\n",
    "- 用调整后的样本，训练下一个基学习器；\n",
    "- 重复上述过程T次，将T个学习器加权结合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.Boosting算法的三要素\n",
    "![images](images/01_06_002.png)\n",
    "- 基学习器\n",
    "- 组合方式\n",
    "- 目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Boosting算法的理论推导与加法模型\n",
    "提升就是指每一步都产生一个弱预测模型，然后加权累加到总模型中，然后每一步弱预测模型生成的依据都是损失函数的负梯度方向(这样才能使得每一步都比前一步的损失小，从而使得最终的总预测模型的损失最小)，这样若干步以后就可以达到逼近损失函数局部最小值的目标，我们知道Boosting算法肯定是一个加法模型，它是由若干个基函数及其权值乘积之和的累加，即\n",
    "$$f(x)=\\sum_{m=1}^M\\beta_mb(x;\\gamma_m)$$\n",
    "其中$b(x;\\gamma_m)$是基函数，$\\beta$是基函数的系数，也就是权重，$\\gamma_m$为基函数的参数，这就是我们最终分类器的样子，现在的目标就是想办法使损失函数的期望取最小值，也就是\n",
    "$$min_{\\beta_m,\\gamma_m}\\sum_{i=1}^NL[y_i,\\sum_{m=1}^m\\beta_mb(x;\\gamma_m)]$$\n",
    "一下子对这M个分类器同时实行优化，显然不太现实，这问题也太复杂了，所以人们想了一个略微折中的办法，因为是加法模型，所以我每一步只对其中一个基函数及其系数进行求解，这样逐步逼近损失函数的最小值，也就是说\n",
    "$$min_{\\beta_m,\\gamma_m}\\sum_{i=1}^NL[y_i,f_{m-1}+\\beta_mb(x;\\gamma_m)]$$\n",
    "要使损失函数最小，那就得使新加的这一项刚好等于损失函数的负梯度，这样不就一步一步使得损失函数最快下降了吗？没错，就是这样，那么就有了\n",
    "$$\\beta_mb(x;\\gamma_m)=-\\lambda\\frac{\\partial{L(y,f_{m-1})}}{\\partial{f}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.前向分步算法\n",
    "1. 初始化$f_0(x)=0$\n",
    "2. 对于m=1,2,...,M有\n",
    "$$\\begin{eqnarray}\n",
    "(\\beta_m,\\gamma_m)=argmin_{\\beta_m,\\gamma_m}\\sum_{i=1}^NL[y_i,\\sum_{m=1}^m\\beta_mb(x;\\gamma_m)]\\\\\n",
    "f_m(x)=f_{m-1}(x)+\\beta_mb(x;\\gamma_m)\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.AdaBoost\n",
    "## 4.1.推导\n",
    "AdaBoost是个分类器，对于分类问题，需要引入指数损失，即\n",
    "$$L[y,f(x)]=e^{-yf(x)}$$\n",
    "令其基函数$b(x;\\gamma)=G(x)$，所以有$G_m(x) \\in [-1,1]$，则在指数损失的基础上，需要解决如下问题\n",
    "$$(\\beta_m,G_m)=argmin_{\\beta,G}\\sum_{i=1}^Ne^{-y_i[f_{m-1}(x_i)+\\beta G(x_i)]}$$\n",
    "令$\\omega_i^{(m)}=e^{-y_if_{m-1}(x_i)}$，则有\n",
    "$$(\\beta_m,G_m)=argmin_{\\beta,G}\\sum_{i=1}^N\\omega_i^{(m)}e^{-y_i\\beta G(x_i)}$$\n",
    "对于二分类问题，有如下规则\n",
    "$$\\begin{eqnarray}\n",
    "y_iG(x_i) &=& 1 & y_i = G(x_i)\\\\\n",
    "y_iG(x_i) &=& -1 & y_i \\neq G(x_i)\n",
    "\\end{eqnarray}$$\n",
    "所以有如下推导：\n",
    "$$\\begin{eqnarray}\n",
    "(\\beta_m,G_m)&=&\\sum_{i=1}^N\\omega_i^{(m)}e^{-y_i\\beta G(x_i)}\\\\\n",
    "&=&e^{-\\beta}\\sum_{y_i=G(x_i)}\\omega_i^{(m)}+e^{\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}\\\\\n",
    "&=&e^{-\\beta}\\sum_{y_i=G(x_i)}\\omega_i^{(m)}+e^{\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)} + e^{-\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)} - e^{-\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}\\\\\n",
    "&=&(e^{\\beta}-e^{-\\beta})\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)} + e^{-\\beta}\\sum_{i=1}^N\\omega_i^{(m)}\\\\\n",
    "&\\because& \\frac{\\partial{e^{\\beta}}}{\\partial{\\beta}} = e^{\\beta} \\\\\n",
    "&\\because& \\frac{\\partial{e^{-\\beta}}}{\\partial{\\beta}} = -e^{-\\beta} \\\\\n",
    "\\Rightarrow \\frac{\\partial{L}}{\\partial{\\beta}} &=& e^{\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)} + e^{-\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)} - e^{-\\beta}\\sum_{i=1}^N\\omega_i^{(m)}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{\\beta}} &=& 0 \\\\\n",
    "&\\Rightarrow& e^{\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)} = e^{-\\beta}\\sum_{i=1}^N\\omega_i^{(m)} - e^{-\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}\\\\\n",
    "&\\Rightarrow& e^{\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)} = e^{-\\beta}[\\sum_{i=1}^N\\omega_i^{(m)} - \\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}]\\\\\n",
    "&\\Rightarrow& loge^{\\beta}\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}=loge^{-\\beta}[\\sum_{i=1}^N\\omega_i^{(m)} - \\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}]\\\\\n",
    "&\\Rightarrow& loge^{\\beta} + log\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)} = loge^{-\\beta} + log[\\sum_{i=1}^N\\omega_i^{(m)} - \\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}]\\\\\n",
    "&\\Rightarrow& loge^{\\beta} - loge^{-\\beta} = log[\\sum_{i=1}^N\\omega_i^{(m)} - \\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}] - log\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}\\\\\n",
    "&\\Rightarrow& \\beta+\\beta= log\\frac{\\sum_{i=1}^N\\omega_i^{(m)} - \\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}}{\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}}\\\\\n",
    "&\\Rightarrow& \\beta=\\frac{1}{2}log\\frac{\\sum_{i=1}^N\\omega_i^{(m)} - \\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}}{\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}}\\\\\n",
    "&\\Rightarrow& \\beta=\\frac{1}{2}log\\frac{\\frac{\\sum_{i=1}^N\\omega_i^{(m)}}{\\sum_{i=1}^N\\omega_i^{(m)}} - \\frac{\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}}{\\sum_{i=1}^N\\omega_i^{(m)}}}{\\frac{\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}}{\\sum_{i=1}^N\\omega_i^{(m)}}}\\\\\n",
    "err &=& \\frac{\\sum_{y_i \\neq G(x_i)}\\omega_i^{(m)}}{\\sum_{i=1}^N\\omega_i^{(m)}}\\\\\n",
    "&\\Rightarrow& \\beta=\\frac{1}{2}log\\frac{1-err}{err}\n",
    "\\end{eqnarray}$$\n",
    "我们可以看到err的分子是误差率，分母是所有样本概率和，那么err是加权的误差率，也可以说是对误差率的归一化。这样我们就求出了基函数在最终预测模型中的权重系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.输入\n",
    "训练数据集$T=\\\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\\\}$,其中$x_i \\in X \\subseteq R^n, y_i \\in Y=\\\\{-1,+1\\\\}$；弱学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.输出\n",
    "最终分类器$G(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.步骤\n",
    "### 4.4.1.初始化训练数据的权重分布\n",
    "$$D_i=(\\omega_{11},...,\\omega_{1i},...,\\omega_{1N}),\\omega_{1i}=\\frac{1}{N}, i=1,2,...,N$$\n",
    "假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中左右相同，这一假设保证这一步能够在原始数据上学习基本分类器$G_1(x)$\n",
    "\n",
    "### 4.4.2.对$m=1,2,...,M$\n",
    "AdaBoost反复学习基本分类器，在每一轮$m=1,2,...,M$顺次执行下列操作\n",
    "1. 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器\n",
    "$$G_m(x):X \\rightarrow \\\\{-1,+1\\\\}$$\n",
    "2. 计算$G_m(x)$在训练数据集上的分类误差率\n",
    "$$e_m=P[G_m(x_i)\\neq y_i]=\\sum_{i=1}^N\\omega_{mi}I[G_m(x_i)\\neq y_i]$$\n",
    "其中\n",
    "$$I[G_m(x_i)\\neq y_i]=\\begin{cases}\n",
    "1 & G_m(x_i) \\neq y_i\\\\\n",
    "0 & G_m(x_i) = y_i\n",
    "\\end{cases}$$\n",
    "这里，$\\omega_{mi}$表示第m轮中第i个实例的权值，$\\sum_{i=1}^N\\omega_{mi}=1$。这表明，$G_m(x)$在加权的训练数据集上的分类误差率是被$G_m(x)$错误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m(x)$的分类误差率的关系\n",
    "3. 计算$G_m(x)$的系数$\\alpha_m$\n",
    "$$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}$$\n",
    "这里的对数是自然对数。$\\alpha_m$表示$G_m(x)$在最终分类器中的重要性，有上述公式可知，当$e_m \\leq \\frac{1}{2}$时，$\\alpha_m \\geq 0$，并且$\\alpha_m$随着$e_m$的减少而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。这个$\\alpha$是当前分类器在最终分类器中的权值\n",
    "4. 更新训练数据集的权值分布为下一轮做准备\n",
    "$$\\begin{eqnarray}\n",
    "D_{m+1}&=&(\\omega_{m+1,1},...,\\omega_{m+1,i},...,\\omega_{m+1,N})\\\\\n",
    "\\omega_{m+1,i}&=&\\frac{\\omega_{mi}}{Z_m}e^{-\\alpha_my_iG_m(x_i)},i=1,2,...,N\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "其中，如果预测正确，那么$y_iG_m(x_i)=1$,否则就是-1。这里，$Z_m$是规范化因子,也就是为了归一化数据\n",
    "$$Z_m=\\sum_{i=1}^N\\omega_{mi}e^{-\\alpha_my_iG_m(x_i)}$$\n",
    "它使得$D_{m+1}$称为一个概率分布。对于上述第二个式子，可以写成\n",
    "$$\\omega_{m+1,i}=\\begin{cases}\n",
    "\\frac{\\omega_{mi}}{Z_m}e^{-\\alpha_m} & G_m(x_i) = y_i\\\\\\\\\n",
    "\\frac{\\omega_{mi}}{Z_m}e^{\\alpha_m} & G_m(x_i) \\neq y_i\n",
    "\\end{cases}$$\n",
    "由此可知，被基本分类器$G_m(x)$错误分类的样本的权值得以扩大，而被正确分类的样本的权值却得以缩小。两相比较，误分类样本的权值被放大$e^{2\\alpha_m}=\\frac{e_m}{1-e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，是的训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点\n",
    "\n",
    "### 4.4.3.构建基本分类器的线性组合\n",
    "$$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$$\n",
    "得到最终分类器\n",
    "$$G(x)=sign[f(x)]=sign[\\sum_{m=1}^M\\alpha_mG_m(x)]$$\n",
    "线性组合$f(x)$实现M个基本分类器的加权表决。系数$\\alpha_m$表示了基本分类器$G_m(x)$的重要性，这里，所有$\\alpha_m$之和并不为1.$f(x)$的符号决定实例$x$的类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。\n",
    "\n",
    "### 4.4.4.需要注意的两点\n",
    "#### 4.4.4.1.$e_m$和$\\alpha_m$\n",
    "首先需要明白的是如果没有任何基本分类器，单凭肉眼去分类，得到的分类误差率应该是0.5，那么有了基本分类器，$e_m$应该是小余0.5的，否则假如分类器没有任何意义，这也就是说$1-e_m > 0.5$，所以$\\alpha_m > 0$的数，每次$e_m$越小，那么$\\alpha_m$就越大。如果$e_m > 0.5$，那么得到的$\\alpha_m$就是一个负值，那么相当于对于此时的分类器$G_m$，我们需要反向考虑。\n",
    "\n",
    "#### 4.4.4.2.更新$\\omega$\n",
    "- 如果第i个样本我们分类错误了，也就是说$G_m(x_i) \\neq y_i$，其中$G_m(x_i)$和$y_i$都是-1或者+1，如果分类错误了，那么$y_iG_m(x_i)$就是-1。\n",
    "- 一般情况下,$\\alpha_m > 0$\n",
    "- 所以$-\\alpha_my_iG_m(x_i)$是一个大于0的数，那么$e^{-\\alpha_my_iG_m(x_i)} > 1$。\n",
    "- 这就是说如果第i个样本分错了，下一个$\\omega_{m+1,i}$就是上一个$\\omega_{mi}$乘以一个大于1的数，相当于权值升高了。\n",
    "- 反之如果分类分对了，权值就会下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.5.分类器误差率$e_m$对其权重$\\alpha_m$的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n  \n/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x110ea3080>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(0, 1, num=10)\n",
    "y = 1/2 * np.log((1-x) / x)\n",
    "\n",
    "plt.figure(facecolor='w')\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以发现：\n",
    "1. $e_m$为0.5时，其权重$\\alpha_m$是0，表示此分类器在最终模型中不起任何作用\n",
    "2. $e_m < 0.5$时其$\\alpha_m > 0$，表示对最终模型起正向作用。$e_m$的值越小，起到的作用越大\n",
    "3. $e_m > 0.5$时其$\\alpha_m < 0$，表示对最终模型起负向作用。$e_m$的值越大，起到的负作用也越大\n",
    "4. $e_m$不会出现等于0的情况，因为到了0的时候，弱分类器已经全部分正确，也就不需要更新权重再次训练了\n",
    "5. $e_m$也不会出现等于1的情况，因为1表示弱分类器全错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.举例\n",
    "给定下列训练样本，试用AdaBoost算法学习一个强分类器。<br/>\n",
    "![images](images/01_06_003.png)<br/>\n",
    "假定我们给定一个线性分类器，是无法把它分开的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1.初始化\n",
    "首先初始化训练数据的权值分布$D_1=\\{\\omega_{11},\\omega_{12},...,\\omega_{1i},...,\\omega_{1N}\\},\\omega_{1i}=\\frac{1}{N},i=1,2,...,N$，由于有10个样本，所以$\\omega_{1i}=0.1$\n",
    "\n",
    "### 4.5.2.第一轮(m=1)\n",
    "我们需要根据样本训练出来一个基本分类器。由于样本很少，我们可以遍历一下1. 我们选择阀值V取2.5是误差率最低，那么基本分类器为\n",
    "$G_1(x)=\\begin{cases}\n",
    "1, & x < 2.5\\\\\\\\\n",
    "-1, & x > 2.5\n",
    "\\end{cases}$，那么我们可以看到只有样本6，7，8做错了\n",
    "2. $G_1(x)$训练集上的误差率$e_1=P(G_1(x_i)\\neq y_i)=P(x=6,7,8)=0.1+0.1+0.1=0.3$。\n",
    "3. 计算$G_1$的系数$\\alpha_1=\\frac{1}{2}log\\frac{1-e_1}{e_1}=\\frac{1}{2}log(\\frac{1-0.3}{0.3})=0.4236$，那么$f_1(x)=0.4236G_1(x)$，分类器$sign[f_1(x)]$在训练数据集上有3个错误分类点\n",
    "4. 更新$D_{m+1}$和$\\omega_{m+1}$，其中\n",
    "$$D_2=(0.0715,0.0715,0.0715,0.0715,0.0715,0.0715,,0.1666,0.1666,0.1666,0.0715)$$\n",
    "这些数是怎么算出来的呢？我们知道公式如下\n",
    "$$\\begin{eqnarray}\n",
    "D_{m+1}&=&(\\omega_{m+1,1},...,\\omega_{m+1,i},...,\\omega_{m+1,N})\\\\\n",
    "\\omega_{m+1,i}&=&\\frac{\\omega_{mi}}{Z_m}e^{-\\alpha_my_iG_m(x_i)},i=1,2,...,N\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "对于我们知道只有$6,7,8$(序号是7，8，9)三个样本分错了，所以$y_7G_7(x_7),y_8G_8(x_8),y_9G_9(x_9)$三个值是-1，其它7个样本的值为1，所以就有\n",
    "$$\\begin{eqnarray}\n",
    "Z_m&=&\\sum_{i=1}^N\\omega_{mi}e^{-\\alpha_my_iG_m(x_i)}\\\\\n",
    "&=&\\frac{1}{10}e^{-0.4236 * 1} * 7 + \\frac{1}{10}e^{-0.4236 * -1} * 3\\\\\n",
    "&=&0.06546857038225139 * 7 + 0.15274504913751732 * 3\\\\\n",
    "&=&0.9165151400883117\n",
    "\\end{eqnarray}$$\n",
    "那么有\n",
    "$$\n",
    "D_2=(\\frac{0.06547}{0.9165},\\frac{0.06547}{0.9165},\\frac{0.06547}{0.9165},\\frac{0.06547}{0.9165},\\frac{0.06547}{0.9165},\\frac{0.06547}{0.9165},\\frac{0.15275}{0.9165},\\frac{0.15275}{0.9165},\\frac{0.15275}{0.9165},\\frac{0.06547}{0.9165})\n",
    "$$\n",
    "看到了吧，第7，8，9号三个样本的权重比别的样本的权重大了，因为它们在这一轮预测错了\n",
    "\n",
    "### 4.5.3.第2轮(m=2)\n",
    "1. 阀值V为8.5误差率最低，故基本分类器为\n",
    "$G_2(x)=\\begin{cases}\n",
    "1, & x < 8.5\\\\\\\\\n",
    "-1, & x > 8.5\n",
    "\\end{cases}$，发现样本3，4，5做错了，那么$e_2=0.0715 * 3 =0.2143,\\alpha_2=0.6496$\n",
    "2. 更新$D_3=(0.0455,0.0455,0.0455,0.1667,0.1667,0.1667,0.1060,0.1060,0.1060,0.0455)$，这时$f_2(x)=0.4236G_1(x)+0.6496G_2(x)$，发现$sign[f_2(x)]$在训练集上有3个错误分类点\n",
    "\n",
    "### 4.5.4.第3轮(m=3)\n",
    "1. 阀值V为5.5是误差率最低，故基本分类器为\n",
    "$G_3(x)=\\begin{cases}\n",
    "1, & x > 5.5\\\\\\\\\n",
    "-1, & x < 5.5\n",
    "\\end{cases}$，发现样本0,1,2,9做错了，错误率就是$e_3=0.0455*4=0.1820$，计算$\\alpha_3=0.7514$\n",
    "2. 更新$D_4=(0.125,0.125,0.125,0.102,0.102,0.102,0.065,0.065,0.065,0.125)$，这时$f_3(x)=0.4236G_1(x)+0.6496G_2(x)+0.7514G_3(x)$。这是发现sign[f_3(x)]在训练集上有0个错误分类点。完美"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.收敛性\n",
    "![images](images/01_06_004.png)\n",
    "![images](images/01_06_005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.总结\n",
    "AdaBoost使用的损失函数为指数函数\n",
    "- 对异常点敏感:指数损失存在的一个问题是不断增加误分类样本的权重（指数上升）。如果数据样本是异常点（outlier），会极大的干扰后面基本分类器学习效果；\n",
    "- 模型无法用于概率估计:对于取值$\\widetilde{A}$的随机变量来说，$e^{-\\widetilde{y}f}$不是任何概率密度函数的对数形式，模型$f(x)$的结果无法用概率解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.提升树(Boosting Tree)与梯度提升树(GBT-Gradient Boosting Tree)\n",
    "## 3.1.概述\n",
    "提升树分为梯度提升决策树GBDT和梯度提升回归树GBRT。提升树算法采用前向分布算法，首先确定初始提升树$f_0(x)=0$，第m步的模型是\n",
    "$$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$$\n",
    "其中，$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\\Theta_m$\n",
    "$$\\hat{\\Theta_m}=argmin_{\\Theta_m}\\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T{x_i;\\Theta_m})$$\n",
    "由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。根据前面的介绍我们了解到根据损失函数的不同分为4种不同的提升方法，包括平方损失函数的回归问题，用指数损失函数的分类问题以及用一般损失函数的一般决策问题。对于二分类问题，只需将AdaBoost算法中的基分类器显示为二分类决策树，就变成了用于二分类问题的提升树，可以说此时的提升树算法是AdaBoost算法的特殊情况。提升树是以决策树为基分类器的提升方法，通常使用CART树。针对不同问题的提升树学习算法，主要区别在于使用的损失函数不同。\n",
    "- 分类问题：指数损失函数。可以使用CART分类树作为AdaBoost的基分类器，此时为分类提升树。\n",
    "- 回归问题：平方误差损失函数。\n",
    "- 决策问题：一般损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.提升树的分类问题\n",
    "对于基函数是分类树时，我们使用指数损失函数，此时正是AdaBoost算法的特殊情况，即将AdaBoost算法中的基分类器使用分类树即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.提升树的回归问题\n",
    "对于基函数是回归树时，我们使用平方差损失函数(残差)，第m棵树的参数为：\n",
    "$$\\hat{\\Theta_m}=argmin_{\\Theta_m}\\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T{x_i;\\Theta_m})$$\n",
    "平方误差损失函数为\n",
    "$$L(y,f(x))=(y-f(x))^2$$\n",
    "那么损失函数就变成了\n",
    "$$\\begin{eqnarray}\n",
    "L(y,f_{m-1}(x)+T(x;\\Theta_m))&=&[y-f_{m-1}(x)-T(x;\\Theta_m)]^2\\\\\n",
    "&=&[r-T(x;\\Theta_m)]^2\n",
    "\\end{eqnarray}$$\n",
    "这里$r=y-f_{m-1}(x)$,可以看到，r时当前模型拟合数据的残差(residual)，随意，对回归问题的提升树算法来说，只需简单的拟合当前模型的残差，这样算法是相当简单的<br/>\n",
    "![images](images/06_BOOST_006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.提升树的决策问题-GBDT(梯度提升决策树Gradient Boosting Decision Tree)\n",
    "提升树利用加法模型与前向分步算法实现学习的优化过程，当损失函数是平方损失和指数损失函数时，每一步的优化是很简单的，但对一般损失函数而言，往往每一步优化并不是十分容易，所以提出了梯度提升算法，这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度\n",
    "\n",
    "### 3.4.1.算法如下\n",
    "$$F_0(\\overrightarrow{x})=argmin_{\\gamma}\\sum_{i=1}^nL(y_i,\\gamma)$$\n",
    "对于m=1到M\n",
    "- 计算伪残差$\\gamma_{im}=[\\frac{\\partial{L(y_i,F(\\overrightarrow{x_i}))}}{\\partial{F(\\overrightarrow{x_i})}}]\\_{F(\\overrightarrow{x})=F_{m-1}(\\overrightarrow{x})}$, i=1,2,...,n\n",
    "- 使用数据$(\\overrightarrow{x_i}, \\gamma_{im})_{i=1}^n$计算拟合残差的基函数$f_m(x)$\n",
    "- 计算步长$\\gamma_m=argmin_{\\gamma}\\sum_{i=1}^nL(y_i,F_{m-1}(\\overrightarrow{x_i})-\\gamma \\bullet f_m(\\overrightarrow{x_i}))$，这是一个一维优化问题\n",
    "- 更新模型$F_m(\\overrightarrow{x})=F_{m-1}(\\overrightarrow{x})-\\gamma_mf_m(\\overrightarrow{x_i})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.提升树的更高级形式-XGBoost(Extreme Gradient Boosting)\n",
    "对于梯度提升树，我们只考虑了一阶导数的形式，如果我们考虑二阶导数，那么就变成了效果更好的XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.XGBoost的模型-树集成(tree ensembles)\n",
    "树集成模型是一组CART(classification and regression trees)树。下面是一个CART的简单实例，它可以分类是否有人喜欢电脑游戏\n",
    "![images](images/01_06_007.png)<br/>\n",
    "我们把一个家庭的成员分成不同的叶子，并把他们分配到相应的叶子节点上。CART与decision trees（决策树）有些许的不同，就是叶子只包含决策值。在CART中，每个叶子都有一个real score（真实的分数），这给了我们更丰富的解释，超越了分类。 这也使得统一的优化步骤更容易。通常情况下，单棵树由于过于简单而不够强大到可以支持在实践中使用的。实际使用的是所谓的tree ensemble model（树集成模型），它将多棵树的预测加到一起。<br/>\n",
    "![images](images/06_BOOST_008.png)<br/>\n",
    "上图是两棵树的集成的例子。将每棵树的预测分数加起来得到最终分数。 如果你看一下这个例子，一个重要的事实就是两棵树互相complement（补充）。在数学表示上，我们可以在表单中编写我们的模型\n",
    "$$\n",
    "\\hat{y}\\_i=\\sum_{k=1}^Kf_k(x_i),f_k \\in F\n",
    "$$\n",
    "其中K是树的数量，f是函数空间F的函数，F是所有可能的CARTs的集合，所以我们优化的目标可以写成\n",
    "$$\n",
    "obj(\\Theta)=\\sum_i^nl(y_i,\\hat{y}\\_i)+\\sum_{k=1}^K\\Omega(f_k)\n",
    "$$\n",
    "那么问题来了，随机森林的模型其实也是树集成，所以随机森林和提升树在模型上并没有不同，不同之处在于我们如何训练他们"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.概述\n",
    "使用一阶导总是会涉及到学习率$\\gamma$，我们考虑二阶导。假设有样本X:$\\{\\overrightarrow{x_1},\\overrightarrow{x_2},...,\\overrightarrow{x_m}\\}$，以及对应的Y个真实值$\\{y_1,y_2,....,y_m\\}$。目前我们已经找到了t-1个决策树\n",
    "$$\\{T_1,T_2,...,T_{t-1}\\}$$\n",
    "以及对应的t-1个学习率\n",
    "$$\\{\\\\alpha_1,\\alpha_2,...,\\alpha_{t-1}\\}$$\n",
    "那么对于任意一个样本$\\overrightarrow{x_i}$，我们总能算出一个预测值\n",
    "$$\\hat{y_i}=\\alpha_1T_1(x_i)+\\alpha_2T_2(x_i)+...+\\alpha_{t-1}T_{t-1}(x_i)$$\n",
    "我们使用符号$\\hat{y}\\_{t-1}^{(i)}$来表示使用t-1棵决策树计算出来的第i个样本的预测值，那么我们就有了一组数据\n",
    "$$\\{(x^{(1)}, \\hat{y}\\_{t-1}^{(1)}), (x^{(2)}, \\hat{y}\\_{t-1}^{(2)}),...,(x^{(m)}, \\hat{y}_{t-1}^{(m)})\\}$$\n",
    "现在我们要考虑的是怎么计算$T_t(X)$以及$\\alpha_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.Taylor展式\n",
    "$$f(x+\\Delta{x}) \\approx f(x)+f'(x)\\Delta{x}+\\frac{1}{2}f''(x)\\Delta{x}^2$$\n",
    "我们可以看出来$\\hat{y}\\_{t-1}^{(i)}$相当于Taylor展式中的x，$f_t(x_i)$相当于$\\Delta{x}$。令\n",
    "$$g_i=\\frac{\\partial{L(y_i, \\hat{y}\\_{t-1}^{(i)})}}{\\partial{\\hat{y}\\_{t-1}^{(i)}}},h_i=\\frac{\\partial^2{L(y_i, \\hat{y}\\_{t-1}^{(i)})}}{\\partial{\\hat{y}\\_{t-1}^{(i)}}}$$\n",
    "由于$\\hat{y}\\_{t-1}^{(i)}$是可以计算出来的，损失函数L是已知的，所以$g_i,h_i$是可以提前计算出来的，所有就有\n",
    "$$J(f_t) \\approx \\sum_{i=1}^n[L(y_i, \\hat{y}_{t-1}^{(i)})+g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega{(f_t)}+C$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.总结\n",
    "- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）\n",
    "- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导\n",
    "- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性 \n",
    "- 在每一步tree boosting之后增加了一个参数n（权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型\n",
    "- 列(特征)抽样，说是从随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法\n",
    "- 并行化处理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
