{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "决策树算法\n",
    "===\n",
    "决策树是分类算法,但是决策树也可以用于求解回归问题，包括ID3、C4.5、C5.0和CART四种算法，前三种属于一种方法的不同改进版本（机器学习的人研究出来的）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.基本概念\n",
    "## 1.1.术语\n",
    "![images](images/01_04_001.png)\n",
    "- 根节点：一颗决策树只有一个根节点\n",
    "- 叶节点：代表一个类别\n",
    "- 中间结点：代表在一个属性上的测试\n",
    "- 分支：代表一个测试输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.决策树的分类过程\n",
    "利用决策树进行分类的过程，其实就是构造树的过程。根据构造子树根节点的不同以及剪枝的不同，可以分为ID3，C4.5，C5.0和CART方法。\n",
    "- ID3： 采用信息增益来划分子树、没有修剪、处理离散特征,基于奥卡姆剃刀原理,(Iterative Dichotomiser 3，迭代二叉树3代)\n",
    "- C4.5：采用信息增益率划分子树、悲观剪枝法、处理离散和连续特征--主流算法\n",
    "- C5.0：采用信息增益率划分子树、自适应增强\n",
    "- CART：采用基尼指数划分子树、代价复杂度剪枝法，处理离散和连续特征--主流算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.举例\n",
    "![images](images/04_DT_002.png)\n",
    "我们采用信息增益来划分子树，所以必须要计算每个特征的信息增益，那么就要求计算条件熵和熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.计算熵\n",
    "总共有17条记录，类别为1的有8条，类别为0的有9条，所以\n",
    "$$\\begin{eqnarray}\n",
    "P(label=1) &=& \\frac{8}{17} = 0.470588235\\\\\n",
    "P(label=0) &=& \\frac{9}{17} = 0.529411765\\\\\n",
    "Info(label)&=& -(0.470588235 * log_2{0.470588235} + 0.529411765 * log_2{0.529411765} = 0.997502546\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.计算每个特征的条件熵以及信息增益\n",
    "### 3.2.1.离散性变量\n",
    "color,root,knocks,texture,navel,touch这六个特征是离散型的\n",
    "\n",
    "#### 3.2.1.1.color特征\n",
    "##### 3.2.1.1.1.dark_green\n",
    "其中数据集中包含dark_green的记录有6条，所以$P(color=dark-green)=\\frac{6}{17}$,类别为1的有3条，类别为0的有3条，所以\n",
    "$$\\begin{eqnarray}\n",
    "P(label=1|color=dark-green) &=& \\frac{3}{6} = 0.5\\\\\n",
    "P(label=0|color=dark-green) &=& \\frac{3}{6} = 0.5\\\\\n",
    "H(color=dark-green) &=& -(0.5 * log_2{0.5} + 0.5 * log_2{0.5}) * \\frac{6}{17} = 0.352941176\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "##### 3.2.1.1.2.black\n",
    "其中数据集中包含black的记录有6条，所以$P(color=black)=\\frac{6}{17}$,类别为1的有4条，类别为0的有2条，所以\n",
    "$$\\begin{eqnarray}\n",
    "P(label=1|color=black) &=& \\frac{4}{6} = 0.666666667\\\\\n",
    "P(label=0|color=black) &=& \\frac{2}{6} = 0.333333333\\\\\n",
    "H(color=black) &=& -(0.67 * log_2{0.67} + 0.337 * log_2(0.337)) * \\frac{6}{17} = 0.324104412\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "##### 3.2.1.1.3.light_white\n",
    "其中数据集中包含light_white的记录有5条，所以$P(color=light-white)=\\frac{5}{17}$,类别为1的有1条，类别为0的有4条，所以\n",
    "$$\\begin{eqnarray}\n",
    "P(label=1|color=light-white) &=& \\frac{1}{5} = 0.2\\\\\n",
    "P(label=0|color=light-white) &=& \\frac{4}{5} = 0.8\\\\\n",
    "H(color=light-white) &=& -(0.2 * log_2{0.2} + 0.8 * log_2{0.8}) * \\frac{5}{17} = 0.212331793\n",
    "\\end{eqnarray}$$\n",
    "所以条件熵:$H(color) = 0.352941176 + 0.324104412 + 0.212331793 = 0.889377381$<br/>\n",
    "信息增益:$Gain(color)=0.997502546 - 0.889377381 = 0.108125165$\n",
    "\n",
    "#### 3.2.1.2.其它离散变量的条件熵和信息增益\n",
    "$$\\begin{eqnarray}\n",
    "H(root) &=& 0.449145413 + 0.405682174 + 0 = 0.854827587\\\\\n",
    "Gain(root) &=& 0.997502546 - 0.854827587 = 0.14267496\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "H(knocks) &=& 0.571147409 + 0.285573704 + 0 = 0.856721113\\\\\n",
    "Gain(knocks) &=& 0.997502546 - 0.856721113 = 0.140781434\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "H(texture) &=& 0.404578856 + 0.212331793 + 0 = 0.616910649\\\\\n",
    "Gain(texture) &=& 0.997502546 - 0.616910649 = 0.380591897\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "H(navel) &=& 0.355402587 + 0.352941176 + 0 = 0.708343764\\\\\n",
    "Gain(navel) &=& 0.997502546 - 0.708343764 = 0.289158783\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "H(touch) &=& 0.705882353 + 0.285573704 + 0 = 0.991456057\\\\\n",
    "Gain(touch) &=& 0.997502546 - 0.991456057 = 0.006046489\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "### 3.2.2.连续性变量\n",
    "density和sugar_ratio是连续型变量\n",
    "\n",
    "#### 3.2.2.1.density变量\n",
    "首先将样本中的density列的数据排序:0.243,0.245,0.343,0.36,0.403,0.437,0.481,0.556,0.593,0.608,0.634,0.639,0.657,0.666,0.697,0.719,0.774\n",
    "\n",
    "然后相邻两数求中间数，有0.244,0.294,0.3515,0.3815,0.42,0.459,0.5185,0.5745,0.6005,0.621,0.6365,0.648,0.6615,0.6815,0.708,0.7465\n",
    "\n",
    "得到16个数字，然后循环这16个数字，做下面的操作\n",
    "- 将样本根据density与当前中间数的大小，将样本分为两部分，比如样本一的density=0.697, 比当前中间数0.244大，那么这条记录归入第一个部分，第二条样本的density=0.774,也比0.244大，也归入第一部分，以此类推，发现只有第10个样本的density=0.243小余0.244，那么它单独归入第二个部分，这样，第一个部分包含16条记录，第二个部分包含1条记录（记录10）\n",
    "- 然后这两个部分分别求熵，然后求和，这个就是当前数字0.244的条件熵=0.941176471\n",
    "- 接着处理第二个数字0.294,方法跟上面一样，最后得到一个列表\n",
    "0.941176471，0.8795220282190911，0.8113643473223249，0.7350632859645522，0.9040038561323909，0.9673004312102036，0.9939174677788095,0.9952755610908235，0.9952755610908235，0.9939174677788095,0.9673004312102036,0.9914560571925497,0.9967327574767078,0.9734165533319407,0.9971690870426205,0.9305406195656446.那么最小值就是0.7350632859645522，这个就是density的条件熵<br/>\n",
    "信息增益=0.997502546 - 0.7350632859645522 = 0.2624392604045631\n",
    "\n",
    "#### 3.2.2.2.sugar_ratio\n",
    "Gain(sugar_ratio)= 0.349293722<br/>\n",
    "综上所述，有\n",
    "$$\\begin{cases}\n",
    "Gain(color)=0.108125165\\\\\\\\\n",
    "Gain(root)=0.14267496\\\\\\\\\n",
    "Gain(knocks)=0.140781434\\\\\\\\\n",
    "Gain(texture)=0.380591897\\\\\\\\\n",
    "Gain(navel)=0.289158783\\\\\\\\\n",
    "Gain(touch)=0.006046489\\\\\\\\\n",
    "Gain(density)=0.2624392604045631\\\\\\\\\n",
    "Gain(sugar-ratio)=0.349293722\n",
    "\\end{cases}$$\n",
    "取每个特征中信息增益最大的，那么就是texture特征，它也就是根节点，它包含三个值distinct、little_blur、blur所以这个树有三个分支"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.递归\n",
    "开始以这三个节点分别为根节点，开始递归\n",
    "\n",
    "### 3.3.1.texture=blur\n",
    "我们发现当texture=blur时，label都是0，那么就证明不需要在进行分支了，它的叶子节点就是0\n",
    "\n",
    "### 3.3.2.texture=distinct\n",
    "用这个数据集作为样本，代入步骤2，进行计算条件熵和信息增益。信息增益分别为(去掉了texture这个特征):0.043068396， 0.458105895, 0.330856225, 0.458105895, 0.458105895, 0.764204507, 0.22478751,可以看到density这个特征的信息增益最大，所以little_blur的子节点就是density，但是它是一个连续变量.我们知道，连续变量离散化的时候，首先需要排序，然后求两两的平均数，得到如下列表0.3015，0.3815，0.42，0.4965，0.582，0.621，0.6655，0.7355,会求出每个值的相关熵，最小的就是0.3185对应的熵，所以我们会用0.3815来作为分割点,<=0.3185的是一个分支，大于的是另一个分支,可以看到<=0.3815的只有第10,15两条记录，且它们的label都是0,大于0.3815的label都是1，所以这个节点下面不需要在递归，一个叶子是0，一个叶子是1\n",
    "\n",
    "### 3.3.3.texture=little_blur\n",
    "同样的，计算结果是它的下面是touch特征，touch下面有分成两个分支，一个0，一个1\n",
    "\n",
    "所以最后的树就是如下：\n",
    "![images](images/01_04_003.png)\n",
    "\n",
    "## 3.4.决策树停止生长的条件\n",
    "- 该群数据的每一笔数据已经归类到每一类数据中，即数据已经不能继续在分。\n",
    "- 该群数据已经找不到新的属性进行节点分割\n",
    "- 该群数据没有任何未处理的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.剪枝\n",
    "## 4.1.理想的决策树\n",
    "1. 叶子节点数最少\n",
    "2. 叶子加点深度最小\n",
    "3. 叶子节点数最少且叶子节点深度最小\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.剪枝的原因-过度拟合问题\n",
    "过度拟合表示，生成的决策树对于本实例匹配的很好，但是由于过度拟合于这个样本，那么对于其它过来的样本，拟合的特别不好，所以我们就说这个决策树过度\n",
    "拟合于当前样本\n",
    "造成多度拟合的潜在原因主要以下两个方面\n",
    "1. 噪声导致的过度拟合\n",
    "2. 缺乏代表性样本所导致的过度拟合\n",
    "\n",
    "所以我们就需要减去一些分支来消除过拟合问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.剪枝的办法\n",
    "### 4.3.1.预剪枝\n",
    "通过提前停止树的构建而对树剪枝，一旦停止，节点就是树叶，该树叶持有子集元祖最频繁的类。停止决策树生长最简单的方法有：\n",
    "- 定义一个高度，当决策树达到该高度时就停止决策树的生长\n",
    "- 达到某个节点的实例具有相同的特征向量，及时这些实例不属于同一类，也可以停止决策树的生长。这个方法对于处理数据的数据冲突问题比较有效。\n",
    "- 定义一个阈值，当达到某个节点的实例个数小于阈值时就可以停止决策树的生长\n",
    "- 定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值大小来决定是否停止决策树的生长。\n",
    "\n",
    "### 4.3.2.后剪枝\n",
    "它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。相比于先剪枝，这种方法更常用，正是因为在先剪枝方法中精确地估计何时停止树增长很困难。\n",
    "\n",
    "- REP-错误率降低剪枝\n",
    "思想：一部分数据用来学习，一部分数据用来测试，它需要对每个节点进行测试，决定该节点是否应该被剪枝：删除以此结点为根的子树、使其成为叶子结点、赋予该结点关联的训练数据的最常见分类、当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点\n",
    "- PEP-悲观错误剪枝 C4.5算法\n",
    "思想：根据剪枝前后的错误率来判定子树的修剪,该方法引入了统计学上连续修正的概念弥补REP中的缺陷，在评价子树的训练错误公式中添加了一个常数，假定每个叶子结点都自动对实例的某个部分进行错误的分类\n",
    "- CCP-代价复杂度剪枝 CART算法\n",
    "\n",
    "> 后剪枝的大致思想就是我们针对一颗子树，尝试将其左右子树(节点)合并，通过测试数据计算合并前后的方差，如果合并后的方差比合并前的小，这说明可以合并此子树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.总结\n",
    "- 使用决策树对样本做分类或回归，是从根节点到叶节点的细化过程；落在相同叶节点的样本的预测值是相同的\n",
    "- 假定某决策树的叶节点数目为T，每个叶节点的权值为$\\overrightarrow{\\omega}=(\\omega_1, \\omega_2,...,\\omega_T)$,决策树的学习过程就是构造如何使用特征得到划分，从而得到这些权值的过程\n",
    "- 样本X落在叶节点q中，定义f为$f_t(X)=\\omega_{q(x)}$，一个决策树的核心即\"树结构\"和\"叶权值\"\n",
    "\n",
    "每个叶节点的权值就是预测值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
