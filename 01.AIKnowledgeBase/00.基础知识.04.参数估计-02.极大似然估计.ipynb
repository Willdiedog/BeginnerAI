{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "极大似然估计\n",
    "===\n",
    "极大似然估计也叫做最大似然估计，是一种估计分布参数的常用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.定义\n",
    "最大似然估计是一种统计方法，它用来求一个样本集的相关概率密度函数的参数。 最大似然法明确地使用概率模型，其目标\n",
    "是寻找能够以较高概率产生观察数据的系统发生树。最大似然法是一类完全基于统计 的系统发生树重建方法的代表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.极大似然问题需要解决的问题\n",
    "## 2.1.概述\n",
    "给定一组数据和一个参数待定的模型，如何确定模型的参数，使得这个确定参数后的模型在所有模型中产生已知数据的概率最大。通俗一点讲，就是在什么情况下最有可能发生已知的事件。举个例子，假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少?当然是70%，这个背后就有极大似然估计的支持。通俗点就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.身高问题\n",
    "假设我们需要调查我们学校的男生和女生的身高分布,怎么做呢？肯定是抽样了。假设你在校园里随便地找了100个男生和100个女生，共200个人（也就是200个身高的样本数据）都在教室里面了。然后男的左边，女的右边。接着就先统计抽样得到的100个男生的身高。假设他们的身高是服从高斯分布的。但是这个分布的均值$\\mu$和方差$\\sigma^2$我们不知道，这两个参数就是我们要估计的。记作$\\theta=[\\mu,\\sigma^2]$。<br/>\n",
    "用数学的语言来说就是：在学校那么多男生（身高）中，我们独立地按照概率密度$p(x|\\theta)$抽取100了个身高，组成样本集X，我们想通过样本集X来估计出未知参数θ。这里概率密度$p(x|\\theta)$我们知道了是高斯分布$N(\\mu,\\sigma^2)$的形式，其中的未知参数是$\\theta=[\\mu,\\sigma^2]$。抽到的样本集是$X=\\{x_1,x_2,…,x_N\\}$，其中$x_i$表示抽到的第i个人的身高，这里N就是100，表示抽到的样本个数。由于每个样本都是独立地从$p(x|\\theta)$中抽取的，换句话说这100个男生中的任何一个，都是随便找的，这些男生之间是没有关系的。那么，学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？因为这些男生（的身高）是服从同一个高斯分布$p(x|\\theta)$的。那么抽到男生A的概率是$p(x_A|\\theta)$，抽到男生B的概率是$p(x_B|\\theta)$，因为他们是独立的，所以很明显，同时抽到男生A和男生B的概率是$p(x_A|\\theta)*p(x_B|\\theta)$，同理，同时抽到这100个男生的概率就是他们各自概率的乘积了。用数学家的口吻说就是用下式表示：\n",
    "$$\n",
    "L(\\theta)=L(x_1,x_2,...,x_n;\\theta)=\\prod_{i=1}^np(x_i;\\theta),\\theta \\in \\Theta\n",
    "$$\n",
    "这个概率反映了，在概率密度函数的参数是$\\theta$时，得到X这组样本的概率。因为这里X是已知的，也就是说我抽取到的这100个人的身高可以测出来，也就是已知的了。而$\\theta$是未知了，则上面这个公式只有$\\theta$是未知数，所以它是$\\theta$的函数。这个函数放映的是在不同的参数$\\theta$取值下，取得当前这个样本集的可能性，因此称为参数$\\theta$相对于样本集X的似然函数（likehood function）。记为$L(\\theta)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.极大似然估计的原理\n",
    "给定一个概率分布D，假定概率密度函数(连续分布)或概率聚集函数(离散分布)为f(D),以及一个分布参数$\\theta$，我们可以从这个分布中抽出一个具有n个值的采样，通过利用f(D)，我们就能计算出其概率:$P(x_1,x_2,...,x_n)=f_D(x_1,x_2,...,x_n|\\theta)$但是我们可能不知道$\\theta$的值，如何对其进行估计呢？可以从这个分布中抽出一个具有n个值的采样$\\{X_1,X_2,...,X_n\\}$，然后用这些采样的数据来估计$\\theta$,这样我们就能找到很多个$\\theta$可能的估计。那么最大似然估计会寻找关于$\\theta$的最有可能的值(即，在所有可能的$\\theta$取值中，寻找一个值使这个采用的可能性最大化)。对于具体问题该如何使用某种分布，通常这个判断来之一些领域内的专家"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.定义\n",
    "无论随机变量X属于离散型分布或者连续性分布，其分布律或概率密度函数为\n",
    "$$P\\{x=x\\}=f(x;\\theta),\\theta \\in \\Theta$$\n",
    "的形式为已知，$\\theta$为待估参数，$\\Theta$是$\\theta$可能取值的范围。设$\\{X_1,X_2,...,X_n\\}$是来自X的样本；则$\\{X_1,X_2,...,X_n\\}$的联合分布律为\n",
    "$$\\prod_{i=1}^nf(x_i;\\theta)$$\n",
    "又设$\\{x_1,x_2,...,x_n\\}$是$\\{X_1,X_2,...,X_n\\}$的一个样本值；即事件$\\{X_1=x_1,...,X_n=x_n\\}$发生的概率为：\n",
    "$$L(\\theta)=L(x_1,x_2,...,x_n;\\theta)=\\prod_{i=1}^nf(x_i;\\theta),\\theta \\in \\Theta$$\n",
    "$L(\\theta)$是叫做样本的似然函数.我们需要找到一个$\\theta$可能的取值$\\hat{\\theta}$，使得似然函数最大,这个值就是极大似然函数的估计值-MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.求解方法\n",
    "- 写出似然函数:$L(\\theta_1,\\theta_2,...,\\theta_k)=L(x_1,...,x_n;\\theta_1,...,\\theta_k)=\\prod_{i=1}^nf(x_i;\\theta_1,\\theta_2,...,\\theta_k)$\n",
    "- 去对数:$lnL(\\theta_1,\\theta_2,...,\\theta_k)=\\sum_{i=1}^nlnf(x_i;\\theta_1,\\theta_2,...,\\theta_k)$\n",
    "- 将对数似然函数对各个参数求偏导并令其为零，得到方程组。\n",
    "- 求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.为何使用对数似然函数\n",
    "因为总概率表达式实际上是很难微分的，所以它几乎总是通过对表达式取自然对数进行简化。因为自然对数是一个单调递增的函数所以，它确保了概率的最大对数值出现在与原始概率函数相同的点上。因此，我们可以用更简单的对数概率来代替原来的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.常见分布的极大似然估计\n",
    "## 6.1.二项分布\n",
    "设$\\{x_1,x_2,...,x_n\\}$是一个样本值。X的分布律为：\n",
    "$$P\\\\{X=x\\\\}=p^x(1-p)^{1-x},x \\in \\{0,1\\}$$\n",
    "解：似然函数为\n",
    "$$L(p)=\\prod_{i=1}^np^{x_i}(1-p)^{1-x_i}=p^{\\sum_{i=1}^nx_i}(1-p)^{n-\\sum_{i=1}^nx_i}$$\n",
    "取对数似然：\n",
    "$$lnL(p)=(\\sum_{i=1}^nx_i)lnp+(n-\\sum_{i=1}^nx_i)ln(1-p)$.令$\\frac{d}{dp}lnL(p)=\\frac{\\sum_{i=1}^nx_i}{p}-\\frac{n-\\sum_{i=1}^nx_i}{1-p}=0$$\n",
    "所以p的极大似然估计量为：$\\hat{p}=\\frac{1}{n}\\sum_{i=1}^nX_i=\\overline{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.$(\\theta+1)x^{\\theta}$\n",
    "设$\\{X_1,X_2,...,X_n\\}$是取自总体X的一个样本，X~\n",
    "$f(x)=\\begin{cases}\n",
    "(\\theta+1)x^{\\theta}, &0<x<1 \\\\\\\\\n",
    "0, &otherwise\n",
    "\\end{cases}$,其中$\\theta>-1$，求$\\theta$的极大似然估计<br/>\n",
    "解：似然函数为$L(\\theta)=\\prod_{i=1}^n(\\theta+1)x_i^{\\theta}=(\\theta+1)^n(\\prod_{i=1}^nx_i)^{\\theta}, 0<x_i<1$,对数似然函数为:\n",
    "$$lnL(\\theta)=nln(\\theta+1)+\\theta\\sum_{i=1}^nlnx_i$$\n",
    "令其导数为0，则有:\n",
    "$$\\frac{dlnL(\\theta)}{d\\theta}=\\frac{n}{\\theta+1}+\\sum_{i=1}^nlnx_i=0$,解得$\\hat{\\theta}=-\\frac{n}{\\sum_{i=1}^nlnx_i}-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.高斯分布\n",
    "设X~N$(\\mu,\\sigma^2)$;$\\mu,\\sigma^2$为未知参数，${x_1,x_2,...,x_n}$是来自X的一个样本值，求$\\mu,\\sigma^2$的极大似然估计量<br/>\n",
    "解：X的概率密度为:\n",
    "$$f(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}$$\n",
    "似然函数为：\n",
    "$$L(\\mu,\\sigma^2)=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}$$\n",
    "对数似然函数为\n",
    "$$lnL(\\mu,\\sigma^2)=-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2$$\n",
    "令\n",
    "$\\begin{cases}\n",
    "\\frac{\\partial{lnL}}{\\partial{\\mu}}=\\frac{1}{\\sigma^2}[\\sum_{i=1}^nx_i-n\\mu]=0 \\\\\\\\\n",
    "\\frac{\\partial{lnL}}{\\partial{\\sigma^2}}=-\\frac{n}{2\\sigma^2}+\\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n(x_i-\\mu)^2=0\n",
    "\\end{cases}$\n",
    "$\\Rightarrow$ \n",
    "$\\begin{cases}\n",
    "\\sum_{i=1}^nx_i=n\\mu \\\\\\\\\n",
    "n\\sigma^2=\\sum_{i=1}^n(x_i-\\mu)^2\n",
    "\\end{cases}$\n",
    "$\\Rightarrow$\n",
    "$\\begin{cases}\n",
    "\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^nx_i=\\overline{x} \\\\\\\\\n",
    "\\sigma^2=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.举例说明\n",
    "假设这次有三个数据点，我们假设它们是从一个被高斯分布充分描述的过程生成的。这些点是9、9.5和11。那么如何用最大似然估计逼近这个高斯分布的参数$\\mu$和$\\sigma$呢?我们知道从高斯分布中生成的单个数据点x的概率是：\n",
    "$$P(x;\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "在我们的例子中，同时观察到这三个数据点的总（联合）概率是：\n",
    "$$\\begin{eqnarray}\n",
    "P(9,9.5,11;\\mu,\\sigma)&=&\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(9-\\mu)^2}{2\\sigma^2}} \\\\\n",
    "&\\times&\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(9.5-\\mu)^2}{2\\sigma^2}}\\\\\n",
    "&\\times&\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(11-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{eqnarray}$$\n",
    "我们只要找出能够让上述表达式最大化的$\\mu,\\sigma$值就可以了,有一种技巧可以帮助我们找到函数的最大值（和最小值）。我们所要做的就是求出函数的导数，把导函数设为零然后重新变换方程，使其参数成为方程的未知数。然后就这样，我们将得到参数的MLE值。我将串讲一下这些步骤，但我假设读者知道如何对常用函数进行微分。但是这么直接求导数会很困难，所以我们使用了对数似然函数来求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.求解\n",
    "对上述表达式量表取对数，有：\n",
    "$$\\begin{eqnarray}\n",
    "lnP(x;\\mu,\\sigma)&=&ln\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{(9-\\mu)^2}{2\\sigma^2}\\\\\n",
    "&+&ln\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{(9.5-\\mu)^2}{2\\sigma^2}\\\\\n",
    "&+&ln\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{(11-\\mu)^2}{2\\sigma^2}\n",
    "\\end{eqnarray}$$\n",
    "对$\\mu$求导，有：\n",
    "$$\\frac{\\partial{lnP(x;\\mu,\\sigma)}}{\\partial{\\mu}}=\\frac{1}{\\sigma^2}[9+9.5+11-3\\mu]$$\n",
    "令等式为0，有：\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial{lnP(x;\\mu,\\sigma)}}{\\partial{\\mu}}=\\frac{1}{\\sigma^2}[9+9.5+11-3\\mu]&=&0\\\\\n",
    "\\mu&=&\\frac{9+9.5+11}{3}=9.833\n",
    "\\end{eqnarray}$$\n",
    "然后求出$\\sigma$即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.总结\n",
    "最小二乘法是另一种常用的机器学习模型参数估计方法。结果表明，当模型向上述例子中一样被假设为高斯分布时，MLE的估计等价于最小二乘法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
