WGAN机器增强版本
===
WGAN=>Wasserstein Generative Adversarial Networks

# 1.原始GAN究竟出了什么问题
原始GAN中判别器要最小化如下损失函数，尽可能把真实样本分为正例，生成样本分为负例：
$$-\mathbb{E}\_{x\sim P_r}\[\log D(x)] - \mathbb{E}\_{x\sim P_g}\[\log(1-D(x))] \tag{1}$$
其中$P_r$是真实样本分布，$P_g$是由生成器产生的样本分布。对于生成器，Goodfellow一开始提出来一个损失函数，后来又提出了一个改进的损失函数，分别是
$$
\begin{eqnarray}
\mathbb{E}\_{x\sim P_g}\[\log(1-D(x))] \tag{2} \\\\
\mathbb{E}_{x\sim P_g}\[- \log D(x)] \tag{3}
\end{eqnarray}
$$
后者在WGAN两篇论文中称为“the - log D alternative”或“the - log D trick”

## 1.1.问题一
**判别器越好，生成器梯度消失越严重**。首先从公式1可以得到，在生成器G固定参数时最优的判别器D应该是什么。对于一个具体的样本x，它可能来自真实分布也可能来自生成分布，它对公式1损失函数的贡献是
$$-P_r(x) \log D(x) - P_g(x) \log \[1 - D(x)]$$
令其关于$D(x)$的导数为0，得
$$-\frac{P_r(x)}{D(x)} + \frac{P_g(x)}{1 - D(x)} = 0$$
化简得最优判别器为：
$$D^*(x) = \frac{P_r(x)}{P_r(x) + P_g(x)} \tag{4} $$
这个结果从直观上很容易理解，就是看一个样本x来自真实分布和生成分布的可能性的相对比例。如果$P_r(x) = 0$且$P_g(x) \neq 0$，最优判别器就应该非常自信地给出概率0；如果$P_r(x) = P_g(x)$，说明该样本是真是假的可能性刚好一半一半，此时最优判别器也应该给出概率0.5。<br/>
然而GAN训练有一个trick，就是别把判别器训练得太好，否则在实验中生成器会完全学不动（loss降不下去），为了探究背后的原因，我们就可以看看在极端情况——判别器最优时，生成器的损失函数变成什么。给公式2加上一个不依赖于生成器的项，使之变成
$$\mathbb{E}\_{x\sim P_r}\[\log D(x)] + \mathbb{E}\_{x\sim P_g}\[\log(1-D(x))]$$
注意，最小化这个损失函数等价于最小化公式2，而且它刚好是判别器损失函数的反。代入最优判别器即公式4，再进行简单的变换可以得到
$$\mathbb{E}\_{x \sim P_r} \log \frac{P_r(x)}{\frac{1}{2}\[P_r(x) + P_g(x)]} + \mathbb{E}_{x \sim P_g} \log \frac{P_g(x)}{\frac{1}{2}\[P_r(x) + P_g(x)]} - 2\log 2 \tag{5} $$
变换成这个样子是为了引入KL散度和JS散度(Jensen-Shannon divergence)这两个重要的相似度衡量指标，后面的主角之一Wasserstein距离，就是要来吊打它们两个的。所以接下来介绍这两个重要的配角——KL散度和JS散度：
$$
\begin{eqnarray}
KL(P_1||P_2)&=&\mathbb{E}\_{x \sim P_1} \log \frac{P_1}{P_2} \tag{6} \\\\
JS(P_1||P_2)&=&\frac{1}{2}KL(P_1||\frac{P_1 + P_2}{2}) + \frac{1}{2}KL(P_2||\frac{P_1 + P_2}{2}) \tag{7}
\end{eqnarray}
$$
于是公式5就可以继续写成
$$2JS(P_r || P_g) - 2\log 2 \tag{8}$$
于是就有如下结论：
> 根据原始GAN定义的判别器loss，我们可以得到最优判别器的形式；而在最优判别器的下，我们可以把原始GAN定义的生成器loss等价变换为最小化真实分布$P_r$与生成分布$P_g$之间的JS散度。我们越训练判别器，它就越接近最优，最小化生成器的loss也就会越近似于最小化$P_r$和$P_g$之间的JS散度。