WGAN机器增强版本
===
WGAN=>Wasserstein Generative Adversarial Networks

# 1.原始GAN究竟出了什么问题
原始GAN中判别器要最小化如下损失函数，尽可能把真实样本分为正例，生成样本分为负例：
$$-\mathbb{E}\_{x\sim P_r}\[\log D(x)] - \mathbb{E}\_{x\sim P_g}\[\log(1-D(x))] \tag{1}$$
其中$P_r$是真实样本分布，$P_g$是由生成器产生的样本分布。对于生成器，Goodfellow一开始提出来一个损失函数，后来又提出了一个改进的损失函数，分别是
$$
\begin{eqnarray}
\mathbb{E}\_{x\sim P_g}\[\log(1-D(x))] \tag{2} \\\\
\mathbb{E}_{x\sim P_g}\[- \log D(x)] \tag{3}
\end{eqnarray}
$$
后者在WGAN两篇论文中称为“the - log D alternative”或“the - log D trick”

## 1.1.问题一
**判别器越好，生成器梯度消失越严重**。首先从公式1可以得到，在生成器G固定参数时最优的判别器D应该是什么。对于一个具体的样本x，它可能来自真实分布也可能来自生成分布，它对公式1损失函数的贡献是
$$-P_r(x) \log D(x) - P_g(x) \log \[1 - D(x)]$$