{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "神经网络配件\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.激活函数\n",
    "## 1.1.常用非线性激励函数-Sigmoid\n",
    "- 正向看，函数$y(x)=\\frac{1}{1+e^{-x}}$，优点是输出数据可以映射到[0,1]之间,很好 \n",
    "- 反向看，$y'(x)=y(x)(1-y(x))$。缺点是梯度下降明显，至少减少75%。这是因为$y'$最大值是0.25，如果梯度原来是1，那么1 * 0.25 = 0.25，那么就是梯度损失了1-25%=75%。所以在深度学习中，最后一层可以使用Sigmoid函数。中间层不可使用，因为梯度损失太厉害"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.常用非线性激励函数-tanh\n",
    "- 正向看，函数$f(x)=tanh(x)=\\frac{2}{1+e^{-2x}}-1$，优点是输出数据可以映射到[-1,1]\n",
    "- 反向看，$f'(x)=1-f(x)^2$。最大值为1,也就是说只有f(x)=0的时候，反向梯度才是1，但是正向为0，也就没有什么激励过程了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.常用非线性激励函数-ReLU(Rectified linear unit)\n",
    "$$f(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases} f'(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$$\n",
    "这个函数正向截断负值，损失大量特征，反向梯度没有损失。由于特征特别多，所以损失一些特征没有关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.常用非线性激励函数-ReLU(Rectified linear unit)\n",
    "$$f(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases} f'(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$$\n",
    "这个函数正向截断负值，损失大量特征，反向梯度没有损失。由于特征特别多，所以损失一些特征没有关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.常用非线性激励函数-Leaky ReLU\n",
    "$$f(x)=\\begin{cases}\n",
    "0.01x & x \\le 0\\\\\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases} f'(x)=\\begin{cases}\n",
    "0.01 & x \\le 0\\\\\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$$\n",
    "优点是保留更多参数，少量梯度反向传播，这个函数可以解决ReLU函数负值神经元失活的问题。你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去。传统神经网络，一般使用Sigmoid或者tanh来做激活函数，而卷积神经网络一般使用ReLU和Leaky ReLU来说激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.损失函数-Loss\n",
    "损失函数是影响深度学习性能最重要的因素之一。是外部世界对神经网络模型训练的直接指导。合适的损失函数能够保证深度学习模型收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Softmax-用于分类问题\n",
    "$$\\sigma(Z)\\_j=\\frac{e^{Z_j}}{\\sum_{k=1}^Ke^{Z_k}}, j=1,2,...,K$$\n",
    "比如特征的目标值是\n",
    "$$[1,2,3,4,1,2,3]$$\n",
    "那么不同值之间的距离比较相近,但是经过损失函数之后变成了\n",
    "$$[0.024,0.064,0.175,0.475,0.024,0.064,0.175]$$\n",
    "这样差别就会很大，这样分类问题的预测结果更明显\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.Cross entropy 交叉熵损失-用于分类问题\n",
    "$$L(w)=\\frac{1}{N}\\sum_{n=1}^NH(p_n,q_n)=-\\frac{1}{N}\\sum_{n=1}^N[y_nlog\\hat{y_n}+(1-y_n)log(1-\\hat{y_n})]$$\n",
    "求偏导有\n",
    "$$\\frac{\\partial{L(w)}}{\\partial{\\omega}}=\\frac{1}{n}\\sum_xx_j(\\sigma{(z)}-y),\\frac{\\partial{C}}{\\partial{b}}=\\frac{1}{n}\\sum_x(\\sigma{(z)}-y)$$\n",
    "说明$\\omega,b$跟激活函数的导数没关系，并且误差越大，梯度就越大，那么参数调整的就越快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.二次代价函数\n",
    "$$L(\\omega)=\\frac{1}{2n}\\sum_x||y(x)-a^L(x)||^2$$\n",
    "L表示代价函数，x表示样本，y表示实际值，a表示输出值，n表示样本总数。如果以一个样本举例，那么有$L=\\frac{(y-a)^2}{2}$，其中$a=\\sigma{(z)}, z=\\sum{\\omega_j*X_j+b}$，其中$\\sigma$是激活函数。如果我们使用梯度下降法来调整权值参数的大小，那么就有\n",
    "$$\\frac{\\partial{C}}{\\partial{\\omega}}=(a-y)\\sigma'(z)x,\\frac{\\partial{C}}{\\partial{b}}=(a-y)\\sigma'(z)$$\n",
    "这说明$\\omega,b$的梯度跟激活函数的梯度成正比，激活函数的梯度越大，$\\omega,b$的大小调整的越快，训练收敛的就越快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.自定义损失函数\n",
    "- 看中某一个属性:单独讲某一些预测值取出活赋予不同大小的参数\n",
    "- 合并多个loss:多目标训练任务，设置合理的loss结合方式\n",
    "- 神经网络融合:不同神经网络loss结合，共同对网络进行训练指导。如果输出神经元是线性的，那么二次代价函数就是一种合适的选择，如果输出神经元是S型函数，那么比较适合用交叉熵代价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.学习率 Learning rate\n",
    "- 数值大：收敛速度快\n",
    "- 数值小：精度高\n",
    "\n",
    "选用合适的学习率的办法\n",
    "- 固定一个\n",
    "- 设置一个step不停迭代\n",
    "- Adagrad\n",
    "- RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.动量\n",
    "正常$x += -LearningRate * dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.过拟合\n",
    "模型的大部分参数能够参与运算，那么过拟合的程度就低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.正则化\n",
    "没有加正则化，就是\n",
    "$$\\begin{eqnarray}\n",
    "Loss=\\hat{y}-y\\\\\n",
    "\\Delta{w}=\\frac{d(Loss)}{d(w)}\\\\\n",
    "w := w - \\eta\\Delta{w}\n",
    "\\end{eqnarray}$$\n",
    "假如正则化以后，\n",
    "$$\\begin{eqnarray}\n",
    "Loss'=\\hat{y}-y+\\lambda \\bullet ||w^2||\\\\\n",
    "\\Delta{w}=\\frac{d(Loss)}{d(w)} + 2\\lambda \\bullet w\\\\\n",
    "w := w-\\eta\\Delta{w}-2\\eta\\lambda{w}\n",
    "\\end{eqnarray}$$\n",
    "其中$2\\eta\\lambda{w}$叫做weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.随机失活 Dropout\n",
    "让神经元以超参数p的概率被激活或者被设置为0，每次随机选择一些神经元进行计算，剩下的不进行计算，这样就可以应对过拟合，因为只有大部分神经元都的参数都接近，才能每次选取不同的神经元才会有好的结果。一般最后两个layers用一下Dropout。训练是使用，测试集不使用。Pooling-对于原始数据进行区域求最大值或者均值的过程，本质就是降维"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
