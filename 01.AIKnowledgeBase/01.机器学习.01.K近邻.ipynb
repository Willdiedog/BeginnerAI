{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "K近邻算法(K Nearest Neighbour)\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 讲义\n",
    "## 1.1.原理\n",
    "数据映射到高维空间中的点、找出K个最近的样本、投票结果<br/>\n",
    "![images](images/01_01_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.如何衡量距离\n",
    "数学中距离满足三个要求：必须是正数、必须对称、满足三角不等式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.常用的距离\n",
    "### 1.3.1.闵可夫斯基距离 Minkowski\n",
    "$$d_{ij}(q)=[\\sum_{k=1}^p(x_{ik}-x_{jk})^q]^{\\frac{1}{q}},q>0$$\n",
    "其中q越大，差异越大的维度对最终距离影响越大。距离是范数的另外一种称呼\n",
    "- 曼哈顿距离:q = 1，城市距离$d_{ij}=|X_1-X_2|+|Y_1-Y_2|$\n",
    "- 欧氏距离：q = 2，直线距离$d_{ij}=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$\n",
    "- 切比雪夫距离:q趋近于无穷大，棋盘距离$d_{ij}=\\lim\\limits_{p\\to\\infty}(\\sum_{i=1}^n|x_i-y_i|^p)^{\\frac{1}{p}}=max|x_i-y_i|$\n",
    "\n",
    "### 1.3.2.马氏距离\n",
    "考虑数据分布<br/>\n",
    "![images](images/01_01_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.具体做法\n",
    "就是计算出测试点与样本中的每个点的距离，找到一个导致最小距离的点，它的类别就是测试点的类别，这个距离的计算，一般使用欧氏距离。K近邻的意思是K个最近的点，如果K=1，那么就是找到离测试点最近的一个样本点，这个样本点是什么分类，那么测试点就是什么分类；如果K>1，那么就是找到离测试点最近的K个点，然后归纳出这些点的分类，看看哪一种分类的样本点最多，那么测试点就是哪一种分类。K越小，越容易发生过拟合<br/>\n",
    "![images](images/01_01_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.三种距离的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMinkowskiDistance(vector1, vector2, q ):\n",
    "    distance = 0.\n",
    "    n = len(vector1)\n",
    "    for i in range(n):\n",
    "        distance += pow(abs(float(vector1[i]) - float(vector2[i])), q)\n",
    "    return round(pow(distance, 1.0 / q), 5)\n",
    "\n",
    "def computeManhattanDistance(vector1, vector2):\n",
    "    return computeMinkowskiDistance(vector1, vector2, 1)\n",
    "\n",
    "def computeEuDistance(vector1, vector2):\n",
    "    return computeMinkowskiDistance(vector1, vector2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.总结\n",
    "KNN相较于其它很多算法一个很不明显不同的地方就是可以通过已有数据计算出结果,而无需进行训练和降低预测误差的反向传播过程,但是在现在的深度学习中这个过程则是必然存在的,所以反向传播的过程则是必然的.而TensorFlow的一个强大之处,就在于此处,对反向传播的自动求导,依靠计算图的机制,在我们使用TensorFlow进行深度学习的开发之时,只需要简单的定义损失函数以及通过一行简简单单的代码,选择要进行优化的损失函数之后,TensorFlow就可以对损失函数进行自动求导,训练我们所构建的计算图"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
