{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵与KL散度\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.信息量\n",
    "信息量用一个信息所需要的编码长度来定义,而一个信息的编码长度跟其出现的概率呈负相关,因为一个短编码的代价也是巨大的,因为会放弃所有以其为前缀的编码方式,比如字母”a”用单一个0作为编码的话,那么为了避免歧义,就不能有其他任何0开头的编码词了.所以一个词出现的越频繁,则其编码方式也就越短,同时付出的代价也大. \n",
    "$$I=log_2\\frac{1}{p(x)}=-log_2p(x)$$\n",
    "一条信息的信息量大小和它的不确定性有直接的关系。比如说，我们要搞清楚一件非常非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，信息量的度量就等于不确定性的多少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.熵\n",
    "熵又被称为信息熵、信源熵、平均自信息量，它表示接受的每条信息中包含的信息的平均量，由于它是由香农从热力学的概念引入到信息论中的，所以也叫做香农熵。它的本质是信息量$log_2\\frac{1}{p}$的期望。如果一个随机变量X的可能取值为$X=\\{x_1,x_2,...,x_n\\}$,对应的概率为$P(X=x_i)$，那么随机变量的熵为\n",
    "$$H(X)=\\sum_iP(X) \\times I=\\sum_iP(X) \\times log_2\\frac{1}{P(X)}=-\\sum_{i=1}^mP(X_i)log_2P(X_i)$$\n",
    "$P(X_i)$表示第i个类别在整个训练元组中出现的概率,熵是一个期望，越不确定，熵越大.事件的结果发生的概率越小，信息量越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.例题一\n",
    "如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。我们无法知道下一个硬币抛掷的结果是什么，因此每一次抛硬币都是不可预测的。因此，使用一枚正常硬币进行若干次抛掷，这个事件的熵是一比特，因为结果不外乎两个——正面或者反面，可以表示为0, 1编码，而且两个结果彼此之间相互独立。若进行n次独立实验，则熵为n，因为可以用长度为n的比特流表示。但是如果一枚硬币的两面完全相同，那个这个系列抛硬币事件的熵等于零，因为结果能被准确预测。现实世界里，我们收集到的数据的熵介于上面两种情况之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.条件熵\n",
    "$$H(X|Y)=H(X,Y)-H(Y)=\\sum_xp(x)H(Y|X=x)$$\n",
    "- X确定时，Y的不确定性度量\n",
    "- 在X发生是前提下，Y发生新带来的熵。\n",
    "\n",
    "比如问今天的下雨概率是多少，如果对这个地方不熟悉，那么概率各50%，这个时候熵是最大的。接下来告诉你这个地方现在是雨季，而且昨天下过雨，天气比较阴，那么这个时候下雨的概率就会更大，比如80%，那么熵会变小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.信息增益（互信息）\n",
    "$$Gain=info(x)-H(X|Y)$$\n",
    "在知道了事件Y之后，X事件不确定性减少程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.信息增益率\n",
    "$$\\mu=\\frac{Gain}{info(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.举例一\n",
    "| 记录号 | 日志密度 | 好友密度 | 是否使用真实头像 | 账号是否真实 |\n",
    "| ----- | ------- | ------- | ------------- | --------- |\n",
    "| 1 | S | S | no | no |\n",
    "| 2 | S | L | yes | yes |\n",
    "| 3 | L | M | yes | yes |\n",
    "| 4 | M | M | yes | yes |\n",
    "| 5 | L | M | yes | yes |\n",
    "| 6 | M | L | no | yes |\n",
    "| 7 | M | S | no | no |\n",
    "| 8 | L | M | no | yes |\n",
    "| 9 | M | S | no | yes |\n",
    "| 10 | S | S | yes | no |\n",
    "\n",
    "其中s、m和l分别表示小、中和大\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.熵\n",
    "针对上面的例子，帐号真实的记录有7条，那么真实账号的概率为$P(X) = 0.7$， 帐号不真实的概率就是0.3。帐号是否真实的熵就是\n",
    "$$0.7 * log_2(0.7) + 0.3 * log_2(0.3) = 0.879$$\n",
    "如果有三个类别，那么就是三个类别中，每个类别的概率乘以以2为底的这个类别的概率的对数，然后相加\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.条件熵\n",
    "根据日志密度来分类，可以分为下面三组数据,第一组数据是日志密度为S的记录\n",
    "\n",
    "| 记录号 | 日志密度 | 好友密度 | 是否使用真实头像 | 账号是否真实 |\n",
    "| ----- | ------- | ------- | ------------- | --------- |\n",
    "| 1 | S | S | no | no |\n",
    "| 2 | S | L | yes | yes |\n",
    "| 10 | S | S | yes | no |\n",
    "\n",
    "三条记录，所以日志密度的概率就是$\\frac{3}{10} = 0.3$,那么P(帐号真实 | 日志密度)的概率就是$\\frac{1}{3}$。所以对于第一组数据得出结果就是日志密度为S的条件熵为$0.3 * (\\frac{2}{3} * log_2\\frac{2}{3} + \\frac{1}{3} * log_2\\frac{1}{3})$\n",
    "\n",
    "| 记录号 | 日志密度 | 好友密度 | 是否使用真实头像 | 账号是否真实 |\n",
    "| ----- | ------- | ------- | ------------- | --------- |\n",
    "| 3 | L | M | yes | yes |\n",
    "| 5 | L | M | yes | yes |\n",
    "| 8 | L | M | no | yes |\n",
    "\n",
    "对于日志是L的第二组记录，它的条件熵就是$0.3 * (\\frac{0}{3} * log_2\\frac{0}{3} + \\frac{3}{3} * log_2\\frac{3}{3})$\n",
    "\n",
    "| 记录号 | 日志密度 | 好友密度 | 是否使用真实头像 | 账号是否真实 |\n",
    "| ----- | ------- | ------- | ------------- | --------- |\n",
    "| 4 | M | M | yes | yes |\n",
    "| 6 | M | L | no | yes |\n",
    "| 7 | M | S | no | no |\n",
    "| 9 | M | S | no | yes |\n",
    "\n",
    "对于日志是M的第三组记录，它的条件熵就是$0.4 * (\\frac{1}{4} * log_2\\frac{1}{4} + \\frac{3}{4} * log_2\\frac{3}{4})$。所以，日志密度的条件熵就是三个数相加，结果就是0.603\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.信息增益\n",
    "日志密度的信息增益就是0.276,同样的办法得到好友密度和是否使用真实头像的信息增益分别是0.033和0.553"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.基尼系数\n",
    "$$Gini(p)=\\sum_{x=1}^Xp_x(1-p_x)=1-\\sum_{x=1}^Xp_x^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.联合熵\n",
    "$$H(X,Y)=−\\sum_{xy}p(x,y)log_2p(x,y)$$\n",
    "X,Y在一起时的不确定性度量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.交叉熵\n",
    "交叉熵本质上可以看成,用一个猜测的分布的编码方式去编码其真实的分布,得到的平均编码长度或者信息量\n",
    "$$H_p(q)=\\sum_xq(x)log_2\\frac{1}{p(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.相对熵\n",
    "相对熵又叫做KL散度，它是行亮两个分布的距离，一般用$D(q||p)$或者$D_q(p)$表示，称作q对p的相对熵\n",
    "$$D_q(p)=H_q(p)-H(p)=\\sum_{k=1}^Np_klog_2\\frac{p_k}{q_k}$$\n",
    "相对熵是非负的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.举例说明\n",
    "小明在学校玩王者荣耀被发现了，爸爸被叫去开家长会，心里悲屈的很，就想法子惩罚小明。到家后，爸爸跟小明说：既然你犯错了，就要接受惩罚，但惩罚的程度就看你聪不聪明了。这样吧，我们俩玩猜球游戏，我拿一个球，你猜球的颜色，你每猜一次，不管对错，你就一个星期不能玩王者荣耀，当然，猜对，游戏停止，否则继续猜。当然，当答案只剩下两种选择时，此次猜测结束后，无论猜对猜错都能100%确定答案，无需再猜一次，此时游戏停止（因为好多人对策略１的结果有疑问，所以请注意这个条件）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.题目一\n",
    "爸爸拿来一个箱子，跟小明说：里面有橙、紫、蓝及青四种颜色的小球任意个，各颜色小球的占比不清楚，现在我从中拿出一个小球，你猜我手中的小球是什么颜色？<br/>小明的策略一：在这种情况下，小明什么信息都不知道，只能认为四种颜色的小球出现的概率是一样的。所以，$\\frac{1}{4}$概率是橙色球，小明需要猜两次，$\\frac{1}{4}$是紫色球，小明需要猜两次，其余的小球类似，所以小明预期的猜球次数为：\n",
    "$$H = \\frac{1}{4} * 2 + \\frac{1}{4} * 2 + \\frac{1}{4} * 2 + \\frac{1}{4} * 2 = 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.题目二\n",
    "爸爸还是拿来一个箱子，跟小明说：箱子里面有小球任意个，但其中$\\frac{1}{2}$是橙色球，$\\frac{1}{4}$是紫色球，$\\frac{1}{8}$是蓝色球及$\\frac{1}{8}$是青色球。我从中拿出一个球，你猜我手中的球是什么颜色的？<br/>\n",
    "小明的策略二：这种情况下，小明知道了每种颜色小球的比例，比如橙色占比二分之一，如果我猜橙色，很有可能第一次就猜中了。所以，$\\frac{1}{2}$的概率是橙色球，小明需要猜一次，$\\frac{1}{4}$的概率是紫色球，小明需要猜两次，$\\frac{1}{8}$的概率是蓝色球，小明需要猜三次，$\\frac{1}{8}$的概率是青色球，小明需要猜三次，所以小明预期的猜题次数为：\n",
    "$$H = \\frac{1}{2} * 1 + \\frac{1}{4} * 2 + \\frac{1}{8} * 3 + \\frac{1}{8} * 3=  1.75$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.题目三\n",
    "其实，爸爸只想让小明意识到自己的错误，并不是真的想罚他，所以拿来一个箱子，跟小明说：里面的球都是橙色，现在我从中拿出一个，你猜我手中的球是什么颜色？肯定是橙色，小明需要猜0次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面三个题目表现出这样一种现象：针对特定概率为p的小球，需要猜球的次数为$log_2\\frac{1}{p}$，这不就是信息量吗。例如题目2中，$\\frac{1}{4}$是紫色球，那么$log_24=2$次,$\\frac{1}{8}$是蓝色球，$\\log_28=3$次。那么针对整个整体，预期的猜题次数为$\\sum_{k=1}^Np_klog_2\\frac{1}{p_k}$，这就是信息熵。那么第一题的信息熵就是2，第二题的信息熵就是1.75，第三题的信息熵就是$1 \\times log_21=0$，由此可见，题目一的熵大于题目二的熵大于题目三的熵，由于信息熵代表的是随机变量或整个系统的不确定性，所以题目一的不确定性最大，实际上也是这样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，每一个系统都会有一个真实的概率分布，也叫真实分布，题目1的真实分布为$(\\frac{1}{4}，\\frac{1}{4}，\\frac{1}{4}，\\frac{1}{4})$，题目2的真实分布为$(\\frac{1}{2}，\\frac{1}{4}，\\frac{1}{8}，\\frac{1}{8})$，而根据真实分布，我们能够找到一个最优策略，以最小的代价消除系统的不确定性，而这个代价大小就是信息熵。记住，信息熵衡量了系统的不确定性，而我们要消除这个不确定性，所要付出的【最小努力】（猜题次数、编码长度等）的大小就是信息熵。具体来讲，题目1只需要猜两次就能确定任何一个小球的颜色，题目2只需要猜测1.75次就能确定任何一个小球的颜色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么交叉熵又是什么呢？回到题目二，如果小明比较笨，对于这道题目，他采用了策略一，即爸爸已经告诉小明这些小球的真实分布是$(\\frac{1}{2}，\\frac{1}{4}，\\frac{1}{8}，\\frac{1}{8})$，但小明所选择的策略却认为所有的小球出现的概率相同，相当于忽略了爸爸告诉小明关于箱子中各小球的真实分布，而仍旧认为所有小球出现的概率是一样的，认为小球的分布为$(\\frac{1}{4}，\\frac{1}{4}，\\frac{1}{4}，\\frac{1}{4})$，这个分布就是非真实分布。此时，小明猜中任何一种颜色的小球都需要猜两次，即\n",
    "$$\\frac{1}{4} * 2 + \\frac{1}{4} * 2 + \\frac{1}{4} * 2 + \\frac{1}{4} * 2 = 2$$\n",
    "但是对于题目二，这是一个坏的策略，因为需要猜题的次数增加了，从1.75变成了2。那么，当我们使用非最优策略消除系统的不确定性，所需要付出的努力的大小我们该如何去衡量呢？这就需要引入交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。正式的讲，交叉熵的公式为：$\\sum_{k=1}^Np_klog_2\\frac{1}{q_k}$,其中$p_k$表示真实分布,$q_k$表示非真实分布。例如上面所讲的将策略1用于题目2，真实分布$p_k=(\\frac{1}{2}，\\frac{1}{4}，\\frac{1}{8}，\\frac{1}{8})$， 非真实分布$q_k=(\\frac{1}{4}，\\frac{1}{4}，\\frac{1}{4}，\\frac{1}{4})$，交叉熵为:\n",
    "$$\\frac{1}{2} \\times log_24 + \\frac{1}{4} \\times log_24 + \\frac{1}{8} \\times log_24 + frac{1}{8} \\times log_24=2$$\n",
    "比最优策略的1.75来得大。因此，交叉熵越低，这个策略就越好，最低的交叉熵也就是使用了真实分布所计算出来的信息熵，因为此时$p_k=q_k$，交叉熵=信息熵。这也是为什么在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们如何去衡量不同策略之间的差异呢？这就需要用到相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异，即：\n",
    "$$KL[f(x) || g(x)]=\\sum_{x \\in X}f(x) \\times log_2\\frac{f(x)}{g(x)}$$\n",
    "现在，假设我们想知道某个策略和最优策略之间的差异，我们就可以用相对熵来衡量这两者之间的差异。即，相对熵=某个策略的交叉熵-信息熵（根据系统真实分布计算而得的信息熵，为最优策略），公式如下：\n",
    "$$KL(p||q)=H(p,q)-H(p)=\\sum_{k=1}^Np_klog_2\\frac{1}{q_k}-\\sum_{k=1}^Np_klog_2\\frac{1}{p_k}=\\sum_{k=1}^Np_klog_2\\frac{p_k}{q_k}$$\n",
    "所以将策略1用于题目2，所产生的相对熵为2 - 1.75 = 0.25."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
