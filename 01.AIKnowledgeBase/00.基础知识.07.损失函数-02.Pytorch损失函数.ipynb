{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pytorch常用的损失函数\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.L1Loss\n",
    "$$Loss(y_i,\\hat{y_i})=|y_i-\\hat{y_i}|$$\n",
    "其实要求维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2627,  0.6468, -0.1317,  0.5093],\n        [-0.9002, -0.0177, -0.4270, -1.3892],\n        [-2.4988, -1.0690,  1.7272,  0.3021]])\ntensor([[ 0.8076,  1.8310, -0.3598,  1.2736],\n        [-0.2235,  1.1021, -0.4336, -1.9632],\n        [-0.1213,  0.6735, -0.6558, -1.0412]])\ntensor([[ 0.4551,  1.1842,  0.2281,  0.7643],\n        [ 0.6766,  1.1198,  0.0066,  0.5740],\n        [ 2.3775,  1.7425,  2.3830,  1.3433]])\ntorch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.L1Loss(reduce=False, size_average=False)\n",
    "input = torch.autograd.Variable(torch.randn(3,4))\n",
    "target = torch.autograd.Variable(torch.randn(3,4))\n",
    "loss = loss_fn(input, target)\n",
    "print(input); print(target); print(loss)\n",
    "print(input.size(), target.size(), loss.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.SmoothL1Loss\n",
    "误差在 (-1,1) 上是平方损失，其他情况是L1损失\n",
    "$$Loss(y_i,\\hat{y_i})=\\begin{cases}\n",
    "\\frac{1}{2}(y_i-\\hat{y_i})^2 & & |y_i-\\hat{y_i}|=1\\\\\n",
    "|y_i-y_i|-\\frac{1}{2} & &otherwise\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0061, -2.6850, -1.4937, -0.3037],\n        [-0.6357, -0.9975, -0.9051, -1.2309],\n        [-0.5110, -1.2267, -0.0272, -0.9562]])\ntensor([[ 0.2577,  0.6965,  0.0527, -0.8612],\n        [-0.1864,  0.1997,  1.2710, -3.0299],\n        [-0.7090,  0.0409,  0.2179, -0.4879]])\ntensor([[ 0.0316,  2.8815,  1.0465,  0.1554],\n        [ 0.1010,  0.6972,  1.6761,  1.2989],\n        [ 0.0196,  0.7676,  0.0301,  0.1096]])\ntorch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.SmoothL1Loss(reduce=False, size_average=False)\n",
    "input = torch.autograd.Variable(torch.randn(3,4))\n",
    "target = torch.autograd.Variable(torch.randn(3,4))\n",
    "loss = loss_fn(input, target)\n",
    "print(input); print(target); print(loss)\n",
    "print(input.size(), target.size(), loss.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.MSELoss\n",
    "均方损失函数\n",
    "$$Loss(y_i,\\hat{y_i})=(y_i-\\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3953, -0.2969, -0.1650,  0.6222],\n        [-0.3519, -0.5679, -0.7107,  2.1467],\n        [ 0.3295, -1.0067, -0.2704,  0.7086]])\ntensor([[-0.5145, -0.2495,  0.1167, -1.5256],\n        [-2.7255, -0.7890, -0.0175,  0.9011],\n        [ 1.1331,  0.4010, -1.9795,  0.3350]])\ntensor([[ 0.0142,  0.0022,  0.0793,  4.6133],\n        [ 5.6340,  0.0489,  0.4806,  1.5515],\n        [ 0.6457,  1.9818,  2.9209,  0.1395]])\ntorch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.MSELoss(reduce=False, size_average=False)\n",
    "input = torch.autograd.Variable(torch.randn(3,4))\n",
    "target = torch.autograd.Variable(torch.randn(3,4))\n",
    "loss = loss_fn(input, target)\n",
    "print(input); print(target); print(loss)\n",
    "print(input.size(), target.size(), loss.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.BCELoss\n",
    "二分类用的交叉损失函数，也可以用于多分类。需要在前面加上Sigmoid激活函数。其target也就是y值需要进行one hot编码，另外BCELoss还可以用于Multi-label classification。我们知道离散版的交叉熵定义为\n",
    "$$H(p,q)=-\\sum_ip_ilog_2q_i$$\n",
    "其中p,q都是向量，且都是概率分布。如果是二分类的话，因为只有正例和反例，且两者的概率和为1，那么只需要预测一个概率就好了，因此可以简化成\n",
    "$$Loss(y_i,\\hat{y_i})=-\\omega_i[\\hat{y_i}log_2y_i+(1-\\hat{y_i})log_2(1-y_i)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3046, -0.0491, -1.1262, -2.1335],\n        [-1.6867,  0.9672, -0.1571,  0.1454],\n        [ 0.1643,  1.9178, -0.3885,  0.1484]])\ntensor([[ 0.,  1.,  0.,  0.],\n        [ 1.,  0.,  1.,  0.],\n        [ 0.,  1.,  0.,  1.]])\ntensor([[ 0.8570,  0.7180,  0.2808,  0.1119],\n        [ 1.8565,  1.2894,  0.7748,  0.7685],\n        [ 0.7787,  0.1371,  0.5176,  0.6217]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "loss_fn = torch.nn.BCELoss(reduce=False, size_average=False)\n",
    "input = torch.autograd.Variable(torch.randn(3, 4))\n",
    "target = torch.autograd.Variable(torch.FloatTensor(3, 4).random_(2))\n",
    "loss = loss_fn(F.sigmoid(input), target)\n",
    "print(input); print(target); print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.BCEWithLogitsLoss\n",
    "结合了Sigmoid和BCELoss，数值结果更加稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.CrossEntropyLoss\n",
    "多分类用的交叉熵损失函数，用这个loss前面不需要加Softmax层。它相当于LogSoftMax + NLLLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.交叉熵损失\n",
    "在[文档](00.基础知识.05.其它-07.熵与KL散度.ipynb)中介绍了交叉熵损失，一般来说交叉熵损失可以用来分类，也可以用于语义分割，对于分类问题，其输出层通常为Sigmoid或Softmax，当然也有可能直接输出加权之后的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.Softmax\n",
    "假设有K个类别，Softmax的计算过程为\n",
    "$$\\sigma(z)_j=\\frac{e^{z_j}}{\\sum_{k=1}^Ke^{z_j}},j=0,1,...,k-1$$\n",
    "![images](images/00_07_02_001.png)<br/>\n",
    "softMax的结果相当于输入图像被分到每个标签的概率分布，该函数是单调增函数，即输入值越大，输出也就越大，输入图像属于该标签的概率就越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.NLLLoss-Negative Log Likelihood\n",
    "用于多分类的负对数似然损失函数\n",
    "$$Loss(x,label)=-X_{label}$$\n",
    "如果需要得到log分布，则需要在网络的最后一层加上LogSoftmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
